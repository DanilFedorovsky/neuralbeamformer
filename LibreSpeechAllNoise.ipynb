{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Module, Linear, Sigmoid, LSTM, BCELoss, MSELoss\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from pytorch_model_summary import summary\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import speechbrain as sb\n",
    "from speechbrain.nnet.losses import get_si_snr_with_pitwrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2703 torch.Size([1, 50000])\n",
      "2703 torch.Size([1, 50000])\n",
      "2703 torch.Size([1, 50000])\n"
     ]
    }
   ],
   "source": [
    "REFERENCE_CHANNEL = 0\n",
    "SAME_LENGTH = True\n",
    "THR_S = 0.5 # Threshold for speech IRM\n",
    "THR_N = 0.5\n",
    "N_PATH = \"/project/data_asr/CHiME5/data/librenoise/free-sound/\"#\"/Users/danilfedorovsky/Documents/10 Collection/00 Studium/00 Letztes Semester/Masterarbeit/Data/noise/free-sound/\"\n",
    "S_PATH = \"/project/data_asr/CHiME5/data/librenoise/dev/dev-clean/\"#\"/Users/danilfedorovsky/Documents/10 Collection/00 Studium/00 Letztes Semester/Masterarbeit/Data/LibriSpeech/dev-clean/\"\n",
    "MODEL_SAVE_PATH = \"/project/data_asr/CHiME5/data/librenoise/models/\"\n",
    "def load_noise(N_PATH=N_PATH):\n",
    "    noise = []\n",
    "    for file in  os.listdir(N_PATH):\n",
    "        if file[-4:] == \".wav\":\n",
    "            sound, _ = torchaudio.load(N_PATH+file)\n",
    "            noise.append(sound)\n",
    "    return noise\n",
    "\n",
    "\n",
    "def load_speech(S_PATH=S_PATH):\n",
    "    speech = []\n",
    "    for folder in  os.listdir(S_PATH):\n",
    "        if os.path.isdir(S_PATH+folder):\n",
    "            for subfolder in os.listdir(S_PATH+folder):\n",
    "                if os.path.isdir(S_PATH+folder+\"/\"+subfolder):\n",
    "                    for file in os.listdir(S_PATH+folder+\"/\"+subfolder):\n",
    "                        if file[-5:] == \".flac\":\n",
    "                            sound, _ = torchaudio.load(S_PATH+folder+\"/\"+subfolder+\"/\"+file)\n",
    "                            if SAME_LENGTH:\n",
    "                                try:\n",
    "                                    sound = torch.narrow(sound,1,0,50000)# Narrow to 50000\n",
    "                                except Exception:\n",
    "                                    # add zeros to make sound 50000 long\n",
    "                                    len_sound = sound.shape[1]\n",
    "                                    add_zeros = 50000 - len_sound\n",
    "                                    add_zeros = torch.zeros(add_zeros).reshape(1,-1)\n",
    "                                    sound = torch.concat([sound,add_zeros],dim=1)\n",
    "                            speech.append(sound)\n",
    "    return speech\n",
    "\n",
    "def add_noise_to_speech(speech, noise, ratio1: float, ratio2: float):\n",
    "    X = []\n",
    "    X2 = []\n",
    "    newNoise = []\n",
    "    for sample in speech:\n",
    "        len_speech = sample.shape[1]\n",
    "        sample_noise = random.choice(noise)\n",
    "        while sample_noise.shape[1]<len_speech:\n",
    "            sample_noise = torch.concat([sample_noise,sample_noise],dim=1)# Repeat to ensure noise is longer than speech\n",
    "        sample_noise = torch.narrow(sample_noise,1,0,len_speech)# Shorten noise to same length as speech\n",
    "        x = torch.add(sample,sample_noise*ratio1)# Same Ratio 1:1\n",
    "        x2 = torch.add(sample,sample_noise*ratio2)# Same Ratio 1:1\n",
    "        sample_noise = torch.narrow(sample_noise,1,0,len_speech)\n",
    "        X.append(x)\n",
    "        X2.append(x)\n",
    "        newNoise.append(sample_noise)\n",
    "    return X, X2, newNoise    \n",
    "\n",
    "def prep_xij(trainX,i,j):\n",
    "    real_part = trainX[i][j].real\n",
    "    imag_part = trainX[i][j].imag\n",
    "    return torch.cat((real_part.unsqueeze(2),imag_part.unsqueeze(2)),2)\n",
    "\n",
    "speech = load_speech()\n",
    "noise = load_noise()\n",
    "noise = noise[:5]\n",
    "X, X2, noise = add_noise_to_speech(speech, noise, 0.1, 0.3)\n",
    "\n",
    "print(len(speech), speech[0].shape)\n",
    "print(len(noise), noise[0].shape)\n",
    "print(len(X), X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([513, 196])\n",
      "torch.Size([513, 196])\n",
      "torch.Size([513, 196])\n"
     ]
    }
   ],
   "source": [
    "# STFT\n",
    "N_FFT = 1024\n",
    "N_HOP = 256\n",
    "\n",
    "stft = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=N_HOP,\n",
    "    power=None,\n",
    ")\n",
    "istft = torchaudio.transforms.InverseSpectrogram(n_fft=N_FFT, hop_length=N_HOP)\n",
    "\n",
    "stfts_mix = []\n",
    "for n in range(0,len(X)):\n",
    "    x_new = torch.concat([stft(X[n]),stft(X2[n])],dim=0)\n",
    "    stfts_mix.append(x_new)\n",
    "\n",
    "    \n",
    "stfts_clean = []\n",
    "for y in speech:\n",
    "    y_new = stft(y)\n",
    "    y_new = y_new.reshape(513,-1)\n",
    "    stfts_clean.append(y_new)\n",
    "\n",
    "stfts_noise = []\n",
    "i = 0\n",
    "for n in noise:\n",
    "    try:\n",
    "        n_new = stft(n)\n",
    "        stfts_noise.append(n_new.reshape(513,-1))\n",
    "    except Exception: #sometimes noises are very short, e.g. noise[697]\n",
    "        continue\n",
    "\n",
    "print(stfts_clean[0].shape)\n",
    "print(stfts_mix[0][0].shape)\n",
    "print(stfts_noise[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irms(stft_clean, stft_noise):\n",
    "    mag_clean = stft_clean.abs() ** 2\n",
    "    mag_noise = stft_noise.abs() ** 2\n",
    "    irm_speech = mag_clean / (mag_clean + mag_noise)\n",
    "    irm_noise = mag_noise / (mag_clean + mag_noise)\n",
    "    return irm_speech[REFERENCE_CHANNEL], irm_noise[REFERENCE_CHANNEL]\n",
    "\n",
    "Y = []\n",
    "for n in range(0,len(noise)):\n",
    "    irm_speech, irm_noise = get_irms(stfts_clean[n].unsqueeze(0), stfts_noise[n].unsqueeze(0))\n",
    "    irm_speech = (irm_speech>THR_S).float()\n",
    "    irm_noise = (irm_noise>THR_N).float()\n",
    "    Y.append(torch.cat((irm_speech.unsqueeze(0),irm_noise.unsqueeze(0)),0))\n",
    "\n",
    "X_h = []\n",
    "stfts_mix_s = torch.stack(stfts_mix)\n",
    "\n",
    "for i in range(0, len(stfts_mix)):\n",
    "    for j in range (0,1):\n",
    "        X_h.append(torch.cat((stfts_mix_s[i][j].real.unsqueeze(0),stfts_mix_s[i][j].imag.unsqueeze(0)),0))\n",
    "\n",
    "speech_s = torch.stack(speech)\n",
    "noise_s = torch.stack(noise)\n",
    "Y = torch.stack(Y)\n",
    "X = torch.stack(X_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskNet + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------\n",
      "      Layer (type)                                         Output Shape         Param #     Tr. Param #\n",
      "========================================================================================================\n",
      "            LSTM-1     [513, 196, 2048], [4, 196, 1024], [4, 196, 1024]      33,603,584      33,603,584\n",
      "          Linear-2                                        [513, 196, 1]           2,049           2,049\n",
      "         Sigmoid-3                                        [513, 196, 1]               0               0\n",
      "========================================================================================================\n",
      "Total params: 33,605,633\n",
      "Trainable params: 33,605,633\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# MASK NET\n",
    "HIDDEN_SIZE=1024 # 256 (128 is too litte, just learns all 0 or 1)\n",
    "SAMPLE_RATE = 16000\n",
    "INPUT_CHANNEL = 2 # Always two -> Real and Imaginary part \n",
    "\n",
    "class MaskNet(Module):\n",
    "    def __init__(self,noise=False):\n",
    "        super(MaskNet, self).__init__()\n",
    "        # First subnet for speech prediction\n",
    "        self.noise = noise\n",
    "        self.lstm = LSTM(input_size=INPUT_CHANNEL, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True)\n",
    "        self.fc = Linear(in_features=HIDDEN_SIZE*2 ,out_features=1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Speech prediction\n",
    "        x = x.reshape(513,-1,2)\n",
    "        self.lstm.flatten_parameters()\n",
    "        y, (h_n, c_n) = self.lstm(x)\n",
    "        y = self.fc(y)\n",
    "        speech_pred = self.sigmoid(y)\n",
    "        return speech_pred#, noise_pred\n",
    "\n",
    "print(summary(MaskNet(),torch.zeros((2, 196, 513))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted on: cuda\n",
      "[INFO] training the network...\n",
      "Epoch: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2000 [00:05<37:58,  1.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb Zelle 8\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi13hpc64/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# zero out the gradients, perform the backpropagation step, and update the weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi13hpc64/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bi13hpc64/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi13hpc64/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi13hpc64/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m H[\u001b[39m\"\u001b[39m\u001b[39mtrain_acc\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(check_accuracy_training(speech_pred,y))\n",
      "File \u001b[0;32m~/anaconda3/envs/beamformer/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/beamformer/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "REFERENCE_CHANNEL = 0\n",
    "INIT_LR = 0.01\n",
    "\n",
    "CUDA = True # if torch.cuda.is_available()\n",
    "device =  torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Mounted on:\", device)\n",
    "\n",
    "lossBCE = BCELoss()\n",
    "#lossSiSNR = get_si_snr_with_pitwrapper([1,],[])\n",
    "model = MaskNet().to(device)\n",
    "model= torch.nn.DataParallel(model)\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "\n",
    "H = {\n",
    "    \"train_loss\":[],\n",
    "    \"train_acc\":[],\n",
    "    \"val_loss\":[],\n",
    "    \"val_acc\":[]\n",
    "}\n",
    "\n",
    "def check_accuracy_training(speech_pred, y_s):\n",
    "    speech_pred = (speech_pred>0.15).float()\n",
    "    return float(torch.sum((speech_pred == y_s).float())/torch.sum(torch.ones(513,speech_pred.shape[1])))\n",
    "\n",
    "def check_accuracy_validation(model):\n",
    "    example_nr = int(np.random.random()*(len(speech)-len(trainX))+len(trainX))\n",
    "    model.eval()\n",
    "    pred = model(X[example_nr]).reshape(1,513,-1)\n",
    "    val_loss = lossBCE(pred,Y[example_nr][0].unsqueeze(0))\n",
    "    pred = (pred>0.15).float()\n",
    "    model.train()\n",
    "    return float(torch.sum((pred == Y[example_nr][0]).float())/torch.sum(torch.ones(513,X[example_nr].shape[2])).to(device)),val_loss\n",
    "\n",
    "# istft = torchaudio.transforms.InverseSpectrogram(n_fft=N_FFT, hop_length=N_HOP).to(device)\n",
    "\n",
    "# def maskToWave(speech_pred,i):\n",
    "#         mix = stfts_mix[i].to(device)\n",
    "#         noise_pred = torch.ones([513,mix.shape[2]]).to(device)-speech_pred\n",
    "#         psd_transform = torchaudio.transforms.PSD()\n",
    "#         psd_speech = psd_transform(mix, speech_pred)\n",
    "#         psd_noise = psd_transform(mix, noise_pred)\n",
    "#         mvdr_transform = torchaudio.transforms.SoudenMVDR()\n",
    "#         stft_souden = mvdr_transform(mix, psd_speech, psd_noise, reference_channel=REFERENCE_CHANNEL)\n",
    "#         waveform_souden = istft(stft_souden, length=len(X[i][0]))#X[i].shape[-1])\n",
    "#         return waveform_souden.reshape(-1)\n",
    "\n",
    "print(\"[INFO] training the network...\")\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    print(\"Epoch:\",str(epoch+1)+\"/\"+str(EPOCHS))\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    trainX = X[:2000]\n",
    "    trainY = Y\n",
    "    for i in tqdm(range(0,len(trainX))): # Iterate over Training Examples\n",
    "        (x, y) = (trainX[i],trainY[i][0].unsqueeze(0))\n",
    "        speech_pred=model(x).reshape(1,513,-1)\n",
    "        #wave = maskToWave(speech_pred,i)\n",
    "        loss = lossBCE(speech_pred,y)\n",
    "        # zero out the gradients, perform the backpropagation step, and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        H[\"train_acc\"].append(check_accuracy_training(speech_pred,y))\n",
    "        H[\"train_loss\"].append(float(loss))\n",
    "        if i % 10 == 0:\n",
    "            val_acc, val_loss = check_accuracy_validation(model)\n",
    "            H[\"val_acc\"].append(val_acc)\n",
    "            H[\"val_loss\"].append(float(val_loss))\n",
    "        if i % 100 == 0:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            print(\"Average Training Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"train_acc\"])))\n",
    "            print(\"Total Training Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"train_loss\"])))\n",
    "            print(\"Average Validation Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"val_acc\"])))\n",
    "            print(\"Total Validation Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"val_loss\"])))\n",
    "            # Save\n",
    "            PATH = \"./models/modelLibre\"\n",
    "            torch.save(model.state_dict(), PATH + str(i) + \".pt\")\n",
    "            # Reset H\n",
    "            H = {\n",
    "                \"train_loss\":[],\n",
    "                \"train_acc\":[],\n",
    "                \"val_loss\":[],\n",
    "                \"val_acc\":[]\n",
    "            }\n",
    "PATH = \"./models/modelLibre\"\n",
    "torch.save(model.state_dict(), PATH + \"final\" + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([513, 196])\n"
     ]
    }
   ],
   "source": [
    "print(Y[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([513, 775])\n"
     ]
    }
   ],
   "source": [
    "print(stfts_mix[4][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_split_size_mb:1024\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb:1024\"\n",
    "print(os.environ['PYTORCH_CUDA_ALLOC_CONF'])\n",
    "\n",
    "del os.environ['PYTORCH_CUDA_ALLOC_CONF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.current_device())\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskNet(\n",
       "  (lstm): LSTM(2, 1024, num_layers=2, bidirectional=True)\n",
       "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"modelLibreepoch9.pt\"\n",
    "device =  torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = MaskNet().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH + MODEL_NAME),strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 513])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_pred = model(X[0])\n",
    "speech_pred.reshape(1,-1,513).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb Zelle 18\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         waveform_souden \u001b[39m=\u001b[39m istft(stft_souden, length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(X[i][\u001b[39m0\u001b[39m]))\u001b[39m#X[i].shape[-1])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m waveform_souden\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m evaluate_example(\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;32m/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb Zelle 18\u001b[0m in \u001b[0;36mevaluate_example\u001b[0;34m(e_nr)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_example\u001b[39m(e_nr):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     speech_pred \u001b[39m=\u001b[39m (model(X[e_nr])\u001b[39m.\u001b[39mreshape(\u001b[39m513\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m>\u001b[39m\u001b[39m0.2\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     noise_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones([\u001b[39m513\u001b[39m,stfts_mix[e_nr]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]])\u001b[39m.\u001b[39mto(device)\u001b[39m-\u001b[39mspeech_pred\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     plot_mask(speech_pred, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrediction Speech\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/beamformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb Zelle 18\u001b[0m in \u001b[0;36mMaskNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m513\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm\u001b[39m.\u001b[39mflatten_parameters()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m y, (h_n, c_n) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(y)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/dev/shm/danil/Beamformer/LibreSpeechAllNoise.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m speech_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(y)\n",
      "File \u001b[0;32m~/anaconda3/envs/beamformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/beamformer/lib/python3.9/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
     ]
    }
   ],
   "source": [
    "def evaluate_example(e_nr):\n",
    "    model.eval()\n",
    "    speech_pred = (model(X[e_nr]).reshape(513,-1)>0.2).float()\n",
    "    noise_pred = torch.ones([513,stfts_mix[e_nr].shape[2]]).to(device)-speech_pred\n",
    "    plot_mask(speech_pred, title=\"Prediction Speech\")\n",
    "    plot_mask(noise_pred, title=\"Prediction Noise\")\n",
    "    plot_mask(trainY[e_nr][0], title=\"Reference Speech\")\n",
    "    plot_mask(trainY[e_nr][1], title=\"Reference Noise\")\n",
    "\n",
    "def plot_mask(mask, title=\"Mask\", xlim=None):\n",
    "    mask = mask.cpu().detach().numpy()\n",
    "    figure, axis = plt.subplots(1, 1)\n",
    "    img = axis.imshow(mask, cmap=\"viridis\", origin=\"lower\", aspect=\"auto\")\n",
    "    figure.suptitle(title)\n",
    "    plt.colorbar(img, ax=axis)\n",
    "    plt.show()\n",
    "\n",
    "def maskToWave(speech_pred,noise_pred,mix,i):\n",
    "        model.eval()\n",
    "        psd_transform = torchaudio.transforms.PSD()\n",
    "        psd_speech = psd_transform(mix[i], speech_pred)\n",
    "        psd_noise = psd_transform(mix[i], noise_pred)\n",
    "        mvdr_transform = torchaudio.transforms.SoudenMVDR()\n",
    "        stft_souden = mvdr_transform(mix[i], psd_speech, psd_noise, reference_channel=REFERENCE_CHANNEL)\n",
    "        waveform_souden = istft(stft_souden, length=len(X[i][0]))#X[i].shape[-1])\n",
    "        return waveform_souden.reshape(-1)\n",
    "\n",
    "evaluate_example(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si-SNR score: -0.38113975524902344\n"
     ]
    }
   ],
   "source": [
    "speech_pred = trainY[0][0]\n",
    "noise_pred = trainY[0][1]\n",
    "\n",
    "def evaluateSiSNR(wave, i):\n",
    "    def si_snr(estimate, reference, epsilon=1e-8):\n",
    "        estimate = estimate - estimate.mean()\n",
    "        reference = reference - reference.mean()\n",
    "        reference_pow = reference.pow(2).mean(axis=1, keepdim=True)\n",
    "        mix_pow = (estimate * reference).mean(axis=1, keepdim=True)\n",
    "        scale = mix_pow / (reference_pow + epsilon)\n",
    "\n",
    "        reference = scale * reference\n",
    "        error = estimate - reference\n",
    "\n",
    "        reference_pow = reference.pow(2)\n",
    "        error_pow = error.pow(2)\n",
    "\n",
    "        reference_pow = reference_pow.mean(axis=1)\n",
    "        error_pow = error_pow.mean(axis=1)\n",
    "\n",
    "        si_snr = 10 * torch.log10(reference_pow) - 10 * torch.log10(error_pow)\n",
    "        return si_snr.item()\n",
    "    score = si_snr(wave, speech[i])\n",
    "    print(f\"Si-SNR score: {score}\") \n",
    "    return score\n",
    "\n",
    "def maskToWave(speech_pred,noise_pred,mix,i):\n",
    "        model.eval()\n",
    "        psd_transform = torchaudio.transforms.PSD()\n",
    "        psd_speech = psd_transform(mix[i], speech_pred)\n",
    "        psd_noise = psd_transform(mix[i], noise_pred)\n",
    "        mvdr_transform = torchaudio.transforms.SoudenMVDR()\n",
    "        stft_souden = mvdr_transform(mix[i], psd_speech, psd_noise, reference_channel=REFERENCE_CHANNEL)\n",
    "        waveform_souden = istft(stft_souden, length=len(X[i][0]))#X[i].shape[-1])\n",
    "        return waveform_souden.reshape(-1)\n",
    "\n",
    "def save_sample(i,wave,sample_rate=SAMPLE_RATE):\n",
    "    model.eval()\n",
    "    wave = wave\n",
    "    torchaudio.save(\"sampleall_output.wav\",wave.reshape(1,-1),sample_rate)\n",
    "    torchaudio.save(\"sampleall_reference.wav\", speech[i].reshape(1,-1),sample_rate)\n",
    "    torchaudio.save(\"sampleall_input.wav\", X[i].reshape(1,-1),sample_rate)\n",
    "\n",
    "\n",
    "sample_nr = 51\n",
    "speech_pred = (model(prep_xij(stfts_mix,sample_nr,0))>0.15).float()\n",
    "noise_pred = torch.ones([513,speech_pred.shape[1]])-speech_pred\n",
    "wave = maskToWave(speech_pred,noise_pred,stfts_mix,sample_nr)\n",
    "save_sample(sample_nr, wave=wave)\n",
    "score = evaluateSiSNR(wave,sample_nr)#Should be ~16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('beamformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7edcfa7c72349f2a40b8bb9d00f805dd689758e4b70f704e502841c884b714f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
