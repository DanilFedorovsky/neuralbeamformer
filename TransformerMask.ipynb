{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "from torchmetrics import SignalNoiseRatio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Module, Linear, Sigmoid, LSTM, BCELoss, MSELoss, Conv1d, Conv2d, MaxPool2d, Transformer, LayerNorm, PReLU, Fold, ConvTranspose1d\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from pytorch_model_summary import summary\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import speechbrain as sb\n",
    "from speechbrain.nnet.losses import get_si_snr_with_pitwrapper\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataLoader\n",
    "\n",
    "X,Y,speech,noise,mix = DataLoader.data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "         Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "             Conv1d-1           [256, 6250]           4,352           4,352\n",
      "          LayerNorm-2           [256, 6250]       3,200,000       3,200,000\n",
      "             Linear-3           [6250, 256]          65,792          65,792\n",
      "        Transformer-4        [250, 49, 256]      11,060,224      11,060,224\n",
      "        Transformer-5        [250, 49, 256]      11,060,224      11,060,224\n",
      "              PReLU-6        [250, 49, 256]               1               1\n",
      "             Linear-7          [49, 128000]   8,192,128,000   8,192,128,000\n",
      "               Fold-8     [512, 1, 1, 6250]               0               0\n",
      "             Linear-9           [6250, 512]         262,656         262,656\n",
      "            Linear-10           [6250, 512]         262,656         262,656\n",
      "   ConvTranspose1d-11         [2, 1, 50000]           4,097           4,097\n",
      "============================================================================\n",
      "Total params: 8,218,048,002\n",
      "Trainable params: 8,218,048,002\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMER MASK NET\n",
    "NUMBER_OF_SPEAKERS = 2\n",
    "ENCODED_TIMESTEPS = int(50000/8) # 50000 is len of training data -> 50000/8 = 6250\n",
    "FOLDS = math.floor((ENCODED_TIMESTEPS/250)*2-1)\n",
    "\n",
    "class TransformerMaskNet(Module):\n",
    "    def __init__(self,noise=False):\n",
    "        super(TransformerMaskNet, self).__init__()\n",
    "        # ENCODER subnet\n",
    "        self.tdnn = Conv1d(in_channels=1,out_channels=256,kernel_size=16,stride=8,padding=6)\n",
    "\n",
    "        self.lnorm = LayerNorm(normalized_shape=(256,ENCODED_TIMESTEPS))\n",
    "        self.lin0 = Linear(in_features=256, out_features=256)\n",
    "\n",
    "        self.tf1 = Transformer(d_model = 256, nhead=8, dim_feedforward=1024)\n",
    "        self.tf2 = Transformer(d_model = 256, nhead=8, dim_feedforward=1024)\n",
    "\n",
    "        self.prelu = PReLU()\n",
    "        self.lin1 = Linear(in_features=64000, out_features=(64000*NUMBER_OF_SPEAKERS))\n",
    "\n",
    "        self.fold = Fold(output_size=(1,ENCODED_TIMESTEPS),kernel_size=(1,250),stride=(1,125))\n",
    "        self.lin2 = Linear(in_features=NUMBER_OF_SPEAKERS*256, out_features=NUMBER_OF_SPEAKERS*256)\n",
    "        self.lin3 = Linear(in_features=NUMBER_OF_SPEAKERS*256, out_features=NUMBER_OF_SPEAKERS*256)\n",
    "\n",
    "        self.convT = ConvTranspose1d(in_channels=256,out_channels=1,kernel_size=16,stride=8, padding=4)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TDNN Encoder 1x50000 -> 256x6250\n",
    "        h = self.tdnn(x)\n",
    "\n",
    "        # NORMALIZATION and Overlapping\n",
    "        x = self.lnorm(h)\n",
    "        x = self.lin0(x.view(-1,256))\n",
    "        x = x.view(256,-1)\n",
    "        x = x.unfold(dimension=1, step=125, size=250)# Chunking and 50% Overlap\n",
    "\n",
    "        # SEPFORMER Block\n",
    "        x = x.reshape(250,FOLDS,256)\n",
    "        y = self.tf1(x,torch.rand(250,FOLDS,256))\n",
    "        x = y + x\n",
    "        y = self.tf2(x,torch.rand(250,FOLDS,256))\n",
    "        x = y + x # Residual connection\n",
    "        \n",
    "        # PRELU and Linear\n",
    "        x = self.prelu(x)\n",
    "        x = x.view(FOLDS,-1)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(256*NUMBER_OF_SPEAKERS,250,FOLDS)\n",
    "\n",
    "        # OVERLAP ADD (256*#S,250,49) -> (256,#S,6250)\n",
    "        x = self.fold(x)\n",
    "        \n",
    "        # FFN + ReLU (256,#S,6250) -> (#S, 256,6250)\n",
    "        x = self.lin2(x.view(-1,NUMBER_OF_SPEAKERS*256))\n",
    "        x = self.lin3(x.view(-1,NUMBER_OF_SPEAKERS*256))\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(NUMBER_OF_SPEAKERS,256,6250)\n",
    "        # DECODER\n",
    "        x = self.convT(x*h)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "print(summary(TransformerMaskNet(),torch.zeros((1, ENCODED_TIMESTEPS*8))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "REFERENCE_CHANNEL = 0\n",
    "INIT_LR = 15e**(âˆ’5)\n",
    "PICKLE_SAVE_PATH = '/project/data_asr/CHiME5/data/librenoise/models/params.pkl'\n",
    "MODEL_SAVE_PATH = '/project/data_asr/CHiME5/data/librenoise/models/TF'\n",
    "\n",
    "CUDA = True # if torch.cuda.is_available()\n",
    "device =  torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Mounted on:\", device)\n",
    "\n",
    "lossBCE = BCELoss().to(device)\n",
    "\n",
    "model = TransformerMaskNet().to(device)\n",
    "model= torch.nn.DataParallel(model,device_ids=[0])\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "\n",
    "H = {\n",
    "    \"train_loss\":[],\n",
    "    \"train_acc\":[],\n",
    "    \"val_loss\":[],\n",
    "    \"val_acc\":[]\n",
    "}\n",
    "\n",
    "def check_accuracy_training(speech_pred, y_s):\n",
    "    speech_pred = (speech_pred>0.15).float()\n",
    "    return float(torch.sum((speech_pred == y_s).float())/torch.sum(torch.ones(513,speech_pred.shape[1])))\n",
    "\n",
    "def check_accuracy_validation(model):\n",
    "    example_nr = int(np.random.random()*(len(speech)-len(trainX))+len(trainX))\n",
    "    model.eval()\n",
    "    pred = model(X[example_nr]).reshape(1,513,-1)\n",
    "    val_loss = lossBCE(pred,Y[example_nr][0].unsqueeze(0))\n",
    "    pred = (pred>0.15).float()\n",
    "    model.train()\n",
    "    return float(torch.sum((pred == Y[example_nr][0]).float())/torch.sum(torch.ones(513,X[example_nr].shape[2])).to(device)),val_loss\n",
    "\n",
    "print(\"[INFO] training the network...\")\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    print(\"Epoch:\",str(epoch+1)+\"/\"+str(EPOCHS))\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    trainX = X[:2000]\n",
    "    trainY = Y\n",
    "    for i in tqdm(range(0,len(trainX))): # Iterate over Training Examples\n",
    "        (x, y) = (trainX[i],trainY[i][0].unsqueeze(0))\n",
    "        speech_pred=model(x)\n",
    "        loss = lossBCE(speech_pred,y)\n",
    "        # zero out the gradients, perform the backpropagation step, and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        #H[\"train_acc\"].append(check_accuracy_training(speech_pred,y))\n",
    "        H[\"train_acc\"].append(check_accuracy_training(speech_pred,y))\n",
    "        H[\"train_loss\"].append(float(loss))\n",
    "        if i % 10 == 0:\n",
    "            val_acc, val_loss = check_accuracy_validation(model)\n",
    "            H[\"val_acc\"].append(val_acc)\n",
    "            H[\"val_loss\"].append(float(val_loss))\n",
    "        if i % 100 == 0:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            print(\"Average Training Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"train_acc\"])))\n",
    "            print(\"Total Training Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"train_loss\"])))\n",
    "            print(\"Average Validation Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"val_acc\"])))\n",
    "            print(\"Total Validation Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"val_loss\"])))\n",
    "    # Save\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH + \"epoch\"+ str(epoch+1) + \".pt\")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH + \"final\" + \".pt\")\n",
    "with open(PICKLE_SAVE_PATH, 'wb') as f:\n",
    "    pickle.dump(H, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('beamformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7edcfa7c72349f2a40b8bb9d00f805dd689758e4b70f704e502841c884b714f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
