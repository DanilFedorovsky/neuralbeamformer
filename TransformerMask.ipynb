{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dfedorovsky/anaconda3/envs/beamformer/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "from torchmetrics import SignalNoiseRatio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Module, Linear, Sigmoid, LSTM, BCELoss, MSELoss, Conv1d, Conv2d, MaxPool2d, Transformer, LayerNorm, PReLU\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from pytorch_model_summary import summary\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import speechbrain as sb\n",
    "from speechbrain.nnet.losses import get_si_snr_with_pitwrapper\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataLoader\n",
    "\n",
    "X,Y,speech,noise,mix = DataLoader.data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 250, 49])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb Zelle 3\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m         speech_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m speech_pred\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(summary(TransformerMaskNet(),torch\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m, \u001b[39m50000\u001b[39;49m))))\n",
      "File \u001b[0;32m~/anaconda3/envs/beamformer/lib/python3.9/site-packages/pytorch_model_summary/model_summary.py:118\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, batch_size, show_input, show_hierarchical, print_summary, max_depth, show_parent_layers, *inputs)\u001b[0m\n\u001b[1;32m    115\u001b[0m model_training \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtraining\n\u001b[1;32m    117\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 118\u001b[0m model(\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m model_training:\n\u001b[1;32m    121\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/beamformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb Zelle 3\u001b[0m in \u001b[0;36mTransformerMaskNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# SEPFORMER Block\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtf1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/beamformer/lib/python3.9/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "# MASK NET\n",
    "NUMBER_OF_SPEAKERS = 2\n",
    "HIDDEN_SIZE=1024 # 1024 (128 is too litte, just learns all 0 or 1)\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "class TransformerMaskNet(Module):\n",
    "    def __init__(self,noise=False):\n",
    "        super(TransformerMaskNet, self).__init__()\n",
    "        # ENCODER subnet\n",
    "        self.tdnn = Conv1d(in_channels=1,out_channels=256,kernel_size=16,stride=8,padding=6)\n",
    "\n",
    "        self.lnorm = LayerNorm(normalized_shape=(256,6250))\n",
    "        self.prelu = PReLU()\n",
    "\n",
    "        self.tf1 = Transformer(d_model = 256, nhead=8, dim_feedforward=1024)\n",
    "        self.tf2 = Transformer(d_model = 256, nhead=8, dim_feedforward=1024)\n",
    "\n",
    "        # self.fc = Linear(in_features=HIDDEN_SIZE*2 ,out_features=1024)\n",
    "        # self.fc2 = Linear(in_features=1024 ,out_features=1024)\n",
    "        # self.fc3 = Linear(in_features=1024 ,out_features=1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TDNN Encoder 1x5000 -> 256x6249\n",
    "        x = self.tdnn(x)\n",
    "\n",
    "        # NORMALIZATION and Overlapping\n",
    "        x = self.lnorm(x)\n",
    "        x = x.unfold(dimension=1, step=125, size=250).reshape(256,250,49)# Chunking and 50% Overlap\n",
    "        print(x.shape)\n",
    "\n",
    "        # SEPFORMER Block\n",
    "        y = self.tf1(x)\n",
    "        y = self.tf2(y)\n",
    "        print(x.shape)\n",
    "        x = y + x # Residual connection\n",
    "\n",
    "        # PRELU and Linear\n",
    "        x = self.prelu(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        speech_pred = self.sigmoid(x)\n",
    "        return speech_pred\n",
    "\n",
    "    \n",
    "\n",
    "print(summary(TransformerMaskNet(),torch.zeros((1, 50000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 250, 49])\n"
     ]
    }
   ],
   "source": [
    "# GOAL NO OVERLAP: 256,250,25\n",
    "# With OVERLAP: 256, 250, 49\n",
    "\n",
    "x  = torch.zeros(256,6250)\n",
    "def chunking(x):\n",
    "    x = x.unfold(dimension=1, step=125, size=250).reshape(256,250,49)\n",
    "    print(x.shape)\n",
    "    #x = torch.stack(torch.tensor_split(input=x, sections=250, dim=1))\n",
    "    #print(x.shape)\n",
    "    #x = x.unfold(dimension=2,step=)\n",
    "    return x\n",
    "    # for filter in x:\n",
    "    #     return torch.tensor_split(input=filter, sections=250,dim=-1)\n",
    "    #     break\n",
    "\n",
    "x = chunking(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 25, 250])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 6250] at entry 0 and [1, 6250] at entry 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb Zelle 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.59/project/data_asr/CHiME5/data/danil/Beamformer/TransformerMask.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mstack(x)\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 6250] at entry 0 and [1, 6250] at entry 6"
     ]
    }
   ],
   "source": [
    "torch.stack(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "REFERENCE_CHANNEL = 0\n",
    "INIT_LR = 15e**(−5)\n",
    "PICKLE_SAVE_PATH = '/project/data_asr/CHiME5/data/librenoise/models/params.pkl'\n",
    "MODEL_SAVE_PATH = '/project/data_asr/CHiME5/data/librenoise/models/TF'\n",
    "\n",
    "CUDA = True # if torch.cuda.is_available()\n",
    "device =  torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Mounted on:\", device)\n",
    "\n",
    "lossBCE = BCELoss().to(device)\n",
    "\n",
    "model = TransformerMaskNet().to(device)\n",
    "model= torch.nn.DataParallel(model,device_ids=[0])\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "\n",
    "H = {\n",
    "    \"train_loss\":[],\n",
    "    \"train_acc\":[],\n",
    "    \"val_loss\":[],\n",
    "    \"val_acc\":[]\n",
    "}\n",
    "\n",
    "def check_accuracy_training(speech_pred, y_s):\n",
    "    speech_pred = (speech_pred>0.15).float()\n",
    "    return float(torch.sum((speech_pred == y_s).float())/torch.sum(torch.ones(513,speech_pred.shape[1])))\n",
    "\n",
    "def check_accuracy_validation(model):\n",
    "    example_nr = int(np.random.random()*(len(speech)-len(trainX))+len(trainX))\n",
    "    model.eval()\n",
    "    pred = model(X[example_nr]).reshape(1,513,-1)\n",
    "    val_loss = lossBCE(pred,Y[example_nr][0].unsqueeze(0))\n",
    "    pred = (pred>0.15).float()\n",
    "    model.train()\n",
    "    return float(torch.sum((pred == Y[example_nr][0]).float())/torch.sum(torch.ones(513,X[example_nr].shape[2])).to(device)),val_loss\n",
    "\n",
    "print(\"[INFO] training the network...\")\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    print(\"Epoch:\",str(epoch+1)+\"/\"+str(EPOCHS))\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    trainX = X[:2000]\n",
    "    trainY = Y\n",
    "    for i in tqdm(range(0,len(trainX))): # Iterate over Training Examples\n",
    "        (x, y) = (trainX[i],trainY[i][0].unsqueeze(0))\n",
    "        speech_pred=model(x)\n",
    "        loss = lossBCE(speech_pred,y)\n",
    "        # zero out the gradients, perform the backpropagation step, and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        #H[\"train_acc\"].append(check_accuracy_training(speech_pred,y))\n",
    "        H[\"train_acc\"].append(check_accuracy_training(speech_pred,y))\n",
    "        H[\"train_loss\"].append(float(loss))\n",
    "        if i % 10 == 0:\n",
    "            val_acc, val_loss = check_accuracy_validation(model)\n",
    "            H[\"val_acc\"].append(val_acc)\n",
    "            H[\"val_loss\"].append(float(val_loss))\n",
    "        if i % 100 == 0:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            print(\"Average Training Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"train_acc\"])))\n",
    "            print(\"Total Training Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"train_loss\"])))\n",
    "            print(\"Average Validation Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"val_acc\"])))\n",
    "            print(\"Total Validation Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"val_loss\"])))\n",
    "    # Save\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH + \"epoch\"+ str(epoch+1) + \".pt\")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH + \"final\" + \".pt\")\n",
    "with open(PICKLE_SAVE_PATH, 'wb') as f:\n",
    "    pickle.dump(H, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('beamformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7edcfa7c72349f2a40b8bb9d00f805dd689758e4b70f704e502841c884b714f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
