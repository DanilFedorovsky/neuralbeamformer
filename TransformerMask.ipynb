{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "from torchmetrics import SignalNoiseRatio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Module, Linear, BCELoss, MSELoss, Conv1d, Conv2d, MaxPool2d, Transformer, LayerNorm, PReLU, Fold, ConvTranspose1d, MultiheadAttention, Dropout\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from pytorch_model_summary import summary\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import speechbrain as sb\n",
    "from speechbrain.nnet.losses import get_si_snr_with_pitwrapper\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataLoader\n",
    "\n",
    "X,Y,speech,noise,mix = DataLoader.data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------\n",
      "            Layer (type)                       Output Shape         Param #     Tr. Param #\n",
      "============================================================================================\n",
      "                Conv1d-1                        [256, 6250]           4,352           4,352\n",
      "             LayerNorm-2                        [256, 6250]       3,200,000       3,200,000\n",
      "                Linear-3                        [6250, 256]          65,792          65,792\n",
      "    PositionalEncoding-4                     [250, 49, 256]               0               0\n",
      "             LayerNorm-5                     [250, 49, 256]       6,272,000       6,272,000\n",
      "    MultiheadAttention-6     [250, 49, 256], [49, 250, 250]         263,168         263,168\n",
      "             LayerNorm-7                     [250, 49, 256]       6,272,000       6,272,000\n",
      "                Linear-8                     [250, 49, 256]          65,792          65,792\n",
      "             LayerNorm-9                     [250, 49, 256]       6,272,000       6,272,000\n",
      "   MultiheadAttention-10     [250, 49, 256], [49, 250, 250]         263,168         263,168\n",
      "            LayerNorm-11                     [250, 49, 256]       6,272,000       6,272,000\n",
      "               Linear-12                     [250, 49, 256]          65,792          65,792\n",
      "            LayerNorm-13                     [250, 49, 256]       6,272,000       6,272,000\n",
      "   MultiheadAttention-14     [250, 49, 256], [49, 250, 250]         263,168         263,168\n",
      "            LayerNorm-15                     [250, 49, 256]       6,272,000       6,272,000\n",
      "               Linear-16                     [250, 49, 256]          65,792          65,792\n",
      "            LayerNorm-17                     [250, 49, 256]       6,272,000       6,272,000\n",
      "   MultiheadAttention-18     [250, 49, 256], [49, 250, 250]         263,168         263,168\n",
      "            LayerNorm-19                     [250, 49, 256]       6,272,000       6,272,000\n",
      "               Linear-20                     [250, 49, 256]          65,792          65,792\n",
      "            LayerNorm-21                     [250, 49, 256]       6,272,000       6,272,000\n",
      "   MultiheadAttention-22     [250, 49, 256], [49, 250, 250]         263,168         263,168\n",
      "            LayerNorm-23                     [250, 49, 256]       6,272,000       6,272,000\n",
      "               Linear-24                     [250, 49, 256]          65,792          65,792\n",
      "            LayerNorm-25                     [250, 49, 256]       6,272,000       6,272,000\n",
      "   MultiheadAttention-26     [250, 49, 256], [49, 250, 250]         263,168         263,168\n",
      "            LayerNorm-27                     [250, 49, 256]       6,272,000       6,272,000\n",
      "               Linear-28                     [250, 49, 256]          65,792          65,792\n",
      "            LayerNorm-29                     [250, 49, 256]       6,272,000       6,272,000\n",
      "   MultiheadAttention-30     [250, 49, 256], [49, 250, 250]         263,168         263,168\n",
      "            LayerNorm-31                     [250, 49, 256]       6,272,000       6,272,000\n",
      "               Linear-32                     [250, 49, 256]          65,792          65,792\n",
      "            LayerNorm-33                     [250, 49, 256]       6,272,000       6,272,000\n",
      "   MultiheadAttention-34     [250, 49, 256], [49, 250, 250]         263,168         263,168\n",
      "            LayerNorm-35                     [250, 49, 256]       6,272,000       6,272,000\n",
      "               Linear-36                     [250, 49, 256]          65,792          65,792\n",
      "                PReLU-37                     [250, 49, 256]               1               1\n",
      "               Linear-38                       [49, 128000]   8,192,128,000   8,192,128,000\n",
      "                 Fold-39                  [512, 1, 1, 6250]               0               0\n",
      "               Linear-40                        [6250, 512]         262,656         262,656\n",
      "               Linear-41                        [6250, 512]         262,656         262,656\n",
      "      ConvTranspose1d-42                      [2, 1, 50000]           4,097           4,097\n",
      "============================================================================================\n",
      "Total params: 8,298,911,234\n",
      "Trainable params: 8,298,911,234\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMER MASK NET\n",
    "NUMBER_OF_SPEAKERS = 2\n",
    "ENCODED_TIMESTEPS = int(50000/8) # 50000 is len of training data -> 50000/8 = 6250\n",
    "FOLDS = math.floor((ENCODED_TIMESTEPS/250)*2-1)\n",
    "\n",
    "class TransformerMaskNet(Module):\n",
    "    def __init__(self,noise=False):\n",
    "        super(TransformerMaskNet, self).__init__()\n",
    "        # ENCODER subnet\n",
    "        self.tdnn = Conv1d(in_channels=1,out_channels=256,kernel_size=16,stride=8,padding=6)\n",
    "\n",
    "        self.lnorm = LayerNorm(normalized_shape=(256,ENCODED_TIMESTEPS))\n",
    "        self.lin0 = Linear(in_features=256, out_features=256)\n",
    "\n",
    "        self.pe = PositionalEncoding(d_model=256)\n",
    "        self.ln11 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln12 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln21 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln22 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln31 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln32 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln41 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln42 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln51 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln52 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln61 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln62 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln71 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln72 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln81 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "        self.ln82 = LayerNorm(normalized_shape=(250,FOLDS,256))\n",
    "\n",
    "        self.lintf1 = Linear(in_features=256,out_features=256)#1024 instead of 256!\n",
    "        self.lintf2 = Linear(in_features=256,out_features=256)\n",
    "        self.lintf3 = Linear(in_features=256,out_features=256)\n",
    "        self.lintf4 = Linear(in_features=256,out_features=256)\n",
    "        self.lintf5 = Linear(in_features=256,out_features=256)\n",
    "        self.lintf6 = Linear(in_features=256,out_features=256)\n",
    "        self.lintf7 = Linear(in_features=256,out_features=256)\n",
    "        self.lintf8 = Linear(in_features=256,out_features=256)\n",
    "\n",
    "        self.mha1 = MultiheadAttention(embed_dim=256,num_heads=8,dropout=0.1)\n",
    "        self.mha2 = MultiheadAttention(embed_dim=256,num_heads=8,dropout=0.1)\n",
    "        self.mha3 = MultiheadAttention(embed_dim=256,num_heads=8,dropout=0.1)\n",
    "        self.mha4 = MultiheadAttention(embed_dim=256,num_heads=8,dropout=0.1)\n",
    "        self.mha5 = MultiheadAttention(embed_dim=256,num_heads=8,dropout=0.1)\n",
    "        self.mha6 = MultiheadAttention(embed_dim=256,num_heads=8,dropout=0.1)\n",
    "        self.mha7 = MultiheadAttention(embed_dim=256,num_heads=8,dropout=0.1)\n",
    "        self.mha8 = MultiheadAttention(embed_dim=256,num_heads=8,dropout=0.1)\n",
    "\n",
    "        self.prelu = PReLU()\n",
    "        self.lin1 = Linear(in_features=64000, out_features=(64000*NUMBER_OF_SPEAKERS))\n",
    "\n",
    "        self.fold = Fold(output_size=(1,ENCODED_TIMESTEPS),kernel_size=(1,250),stride=(1,125))\n",
    "        self.lin2 = Linear(in_features=NUMBER_OF_SPEAKERS*256, out_features=NUMBER_OF_SPEAKERS*256)\n",
    "        self.lin3 = Linear(in_features=NUMBER_OF_SPEAKERS*256, out_features=NUMBER_OF_SPEAKERS*256)\n",
    "\n",
    "        self.convT = ConvTranspose1d(in_channels=256,out_channels=1,kernel_size=16,stride=8, padding=4)\n",
    "\n",
    "        #self.tf1 = Transformer(d_model = 256, nhead=8, dim_feedforward=1024)\n",
    "        #self.tf2 = Transformer(d_model = 256, nhead=8, dim_feedforward=1024)\n",
    "        # SEPFORMER Block\n",
    "        # y = self.tf1(x,torch.rand(250,FOLDS,256))\n",
    "        # x = y + x\n",
    "        # y = self.tf2(x,torch.rand(250,FOLDS,256))\n",
    "        # x = y + x # Residual connection\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TDNN Encoder 1x50000 -> 256x6250\n",
    "        h = self.tdnn(x)\n",
    "\n",
    "        # NORMALIZATION and Overlapping\n",
    "        x = self.lnorm(h)\n",
    "        x = self.lin0(x.view(-1,256))\n",
    "        x = x.view(256,-1)\n",
    "        x = x.unfold(dimension=1, step=125, size=250)# Chunking and 50% Overlap\n",
    "\n",
    "        x = x.reshape(250,FOLDS,256)\n",
    "\n",
    "        # Transformer 1\n",
    "        y = self.pe(x)\n",
    "        z = self.ln11(y)\n",
    "        z, _ = self.mha1(z,z,z)\n",
    "        z_2 = z+y\n",
    "        z = self.ln12(z_2)\n",
    "        z = self.lintf1(z)\n",
    "        x = z+z_2+x\n",
    "        # Transformer 2\n",
    "        y = self.pe(x)\n",
    "        z = self.ln21(y)\n",
    "        z, _ = self.mha2(z,z,z)\n",
    "        z_2 = z+y\n",
    "        z = self.ln22(z_2)\n",
    "        z = self.lintf2(z)\n",
    "        x = z+z_2+x\n",
    "        # Transformer 3\n",
    "        y = self.pe(x)\n",
    "        z = self.ln31(y)\n",
    "        z, _ = self.mha3(z,z,z)\n",
    "        z_2 = z+y\n",
    "        z = self.ln32(z_2)\n",
    "        z = self.lintf3(z)\n",
    "        x = z+z_2+x\n",
    "        # Transformer 4\n",
    "        y = self.pe(x)\n",
    "        z = self.ln41(y)\n",
    "        z, _ = self.mha4(z,z,z)\n",
    "        z_2 = z+y\n",
    "        z = self.ln42(z_2)\n",
    "        z = self.lintf4(z)\n",
    "        x = z+z_2+x\n",
    "        # Transformer 5\n",
    "        y = self.pe(x)\n",
    "        z = self.ln51(y)\n",
    "        z, _ = self.mha5(z,z,z)\n",
    "        z_2 = z+y\n",
    "        z = self.ln52(z_2)\n",
    "        z = self.lintf5(z)\n",
    "        x = z+z_2+x\n",
    "        # Transformer 6\n",
    "        y = self.pe(x)\n",
    "        z = self.ln61(y)\n",
    "        z, _ = self.mha6(z,z,z)\n",
    "        z_2 = z+y\n",
    "        z = self.ln62(z_2)\n",
    "        z = self.lintf6(z)\n",
    "        x = z+z_2+x\n",
    "        # Transformer 7\n",
    "        y = self.pe(x)\n",
    "        z = self.ln71(y)\n",
    "        z, _ = self.mha7(z,z,z)\n",
    "        z_2 = z+y\n",
    "        z = self.ln72(z_2)\n",
    "        z = self.lintf7(z)\n",
    "        x = z+z_2+x\n",
    "        # Transformer 8\n",
    "        y = self.pe(x)\n",
    "        z = self.ln81(y)\n",
    "        z, _ = self.mha8(z,z,z)\n",
    "        z_2 = z+y\n",
    "        z = self.ln82(z_2)\n",
    "        z = self.lintf8(z)\n",
    "        x = z+z_2+x\n",
    "\n",
    "        # PRELU and Linear\n",
    "        x = self.prelu(x)\n",
    "        x = x.view(FOLDS,-1)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(256*NUMBER_OF_SPEAKERS,250,FOLDS)\n",
    "\n",
    "        # OVERLAP ADD (256*#S,250,49) -> (256,#S,6250)\n",
    "        x = self.fold(x)\n",
    "        \n",
    "        # FFN + ReLU (256,#S,6250) -> (#S, 256,6250)\n",
    "        x = self.lin2(x.view(-1,NUMBER_OF_SPEAKERS*256))\n",
    "        x = self.lin3(x.view(-1,NUMBER_OF_SPEAKERS*256))\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(NUMBER_OF_SPEAKERS,256,6250)\n",
    "        # DECODER\n",
    "        x = self.convT(x*h)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "print(summary(TransformerMaskNet(),torch.zeros((1, ENCODED_TIMESTEPS*8))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "REFERENCE_CHANNEL = 0\n",
    "INIT_LR = 0.001#0.000015\n",
    "PICKLE_SAVE_PATH = '/project/data_asr/CHiME5/data/librenoise/models/params.pkl'\n",
    "MODEL_SAVE_PATH = '/project/data_asr/CHiME5/data/librenoise/models/TF'\n",
    "\n",
    "CUDA = True # if torch.cuda.is_available()\n",
    "device =  torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Mounted on:\", device)\n",
    "\n",
    "lossBCE = BCELoss().to(device)\n",
    "\n",
    "model = TransformerMaskNet().to(device)\n",
    "model= torch.nn.DataParallel(model,device_ids=[0])\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "\n",
    "H = {\n",
    "    \"train_loss\":[],\n",
    "    \"train_acc\":[],\n",
    "    \"val_loss\":[],\n",
    "    \"val_acc\":[]\n",
    "}\n",
    "\n",
    "def check_accuracy_training(speech_pred, y_s):\n",
    "    speech_pred = (speech_pred>0.15).float()\n",
    "    return float(torch.sum((speech_pred == y_s).float())/torch.sum(torch.ones(513,speech_pred.shape[1])))\n",
    "\n",
    "def check_accuracy_validation(model):\n",
    "    example_nr = int(np.random.random()*(len(speech)-len(trainX))+len(trainX))\n",
    "    model.eval()\n",
    "    pred = model(X[example_nr]).reshape(1,513,-1)\n",
    "    val_loss = lossBCE(pred,Y[example_nr][0].unsqueeze(0))\n",
    "    pred = (pred>0.15).float()\n",
    "    model.train()\n",
    "    return float(torch.sum((pred == Y[example_nr][0]).float())/torch.sum(torch.ones(513,X[example_nr].shape[2])).to(device)),val_loss\n",
    "\n",
    "print(\"[INFO] training the network...\")\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    print(\"Epoch:\",str(epoch+1)+\"/\"+str(EPOCHS))\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    trainX = X[:2000]\n",
    "    trainY = Y\n",
    "    for i in tqdm(range(0,len(trainX))): # Iterate over Training Examples\n",
    "        (x, y) = (trainX[i],trainY[i][0].unsqueeze(0))\n",
    "        speech_pred=model(x)\n",
    "        loss = lossBCE(speech_pred,y)\n",
    "        # zero out the gradients, perform the backpropagation step, and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        #H[\"train_acc\"].append(check_accuracy_training(speech_pred,y))\n",
    "        H[\"train_acc\"].append(check_accuracy_training(speech_pred,y))\n",
    "        H[\"train_loss\"].append(float(loss))\n",
    "        if i % 10 == 0:\n",
    "            val_acc, val_loss = check_accuracy_validation(model)\n",
    "            H[\"val_acc\"].append(val_acc)\n",
    "            H[\"val_loss\"].append(float(val_loss))\n",
    "        if i % 100 == 0:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            print(\"Average Training Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"train_acc\"])))\n",
    "            print(\"Total Training Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"train_loss\"])))\n",
    "            print(\"Average Validation Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"val_acc\"])))\n",
    "            print(\"Total Validation Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"val_loss\"])))\n",
    "    # Save\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH + \"epoch\"+ str(epoch+1) + \".pt\")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH + \"final\" + \".pt\")\n",
    "with open(PICKLE_SAVE_PATH, 'wb') as f:\n",
    "    pickle.dump(H, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('beamformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7edcfa7c72349f2a40b8bb9d00f805dd689758e4b70f704e502841c884b714f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
