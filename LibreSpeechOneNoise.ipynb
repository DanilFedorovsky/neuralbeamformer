{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilfedorovsky/miniforge3/envs/pytorch/lib/python3.9/site-packages/torchaudio/_internal/module_utils.py:99: UserWarning: Failed to import soundfile. 'soundfile' backend is not available.\n",
      "  warnings.warn(\"Failed to import soundfile. 'soundfile' backend is not available.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Module, Linear, Sigmoid, LSTM, MSELoss\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from pytorch_model_summary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([513, 196])\n",
      "torch.Size([513, 196])\n",
      "torch.Size([513, 196])\n"
     ]
    }
   ],
   "source": [
    "REFERENCE_CHANNEL = 0\n",
    "N_PATH = \"/Users/danilfedorovsky/Documents/10 Collection/00 Studium/00 Letztes Semester/Masterarbeit/Data/noise/free-sound/\"\n",
    "S_PATH = \"/Users/danilfedorovsky/Documents/10 Collection/00 Studium/00 Letztes Semester/Masterarbeit/Data/LibriSpeech/dev-clean/\"\n",
    "def load_noise(N_PATH=N_PATH):\n",
    "    noise = []\n",
    "    for file in  os.listdir(N_PATH):\n",
    "        if file[-4:] == \".wav\":\n",
    "            sound, _ = torchaudio.load(N_PATH+file)\n",
    "            noise.append(sound)\n",
    "    return noise\n",
    "\n",
    "\n",
    "def load_speech(S_PATH=S_PATH):\n",
    "    speech = []\n",
    "    for folder in  os.listdir(S_PATH):\n",
    "        if os.path.isdir(S_PATH+folder):\n",
    "            for subfolder in os.listdir(S_PATH+folder):\n",
    "                if os.path.isdir(S_PATH+folder+\"/\"+subfolder):\n",
    "                    for file in os.listdir(S_PATH+folder+\"/\"+subfolder):\n",
    "                        if file[-5:] == \".flac\":\n",
    "                            sound, _ = torchaudio.load(S_PATH+folder+\"/\"+subfolder+\"/\"+file)\n",
    "                            try:\n",
    "                                sound = torch.narrow(sound,1,0,50000)# Narrow to 50000\n",
    "                            except Exception:\n",
    "                                # add zeros to make sound 50000 long\n",
    "                                len_sound = sound.shape[1]\n",
    "                                add_zeros = 50000 - len_sound\n",
    "                                add_zeros = torch.zeros(add_zeros).reshape(1,-1)\n",
    "                                sound = torch.concat([sound,add_zeros],dim=1)\n",
    "                            speech.append(sound)\n",
    "    return speech\n",
    "\n",
    "\n",
    "def add_noise_to_speech(speech, noise):\n",
    "    X = []\n",
    "    for sample in speech:\n",
    "        len_speech = sample.shape[1]\n",
    "        sample_noise = torch.concat([noise[6],noise[6],noise[6],noise[6]],dim=1)# Repeat to ensure noise is longer than speech\n",
    "        sample_noise = torch.narrow(sample_noise,1,0,len_speech)# Shorten noise to same length as speech\n",
    "        x = torch.add(sample,sample_noise*0.2)# Same Ratio 1:1\n",
    "        X.append(x)\n",
    "    return X    \n",
    "\n",
    "def get_one_noise(speech, noise):# Make Noise 6 same length as respective speech\n",
    "    X = []\n",
    "    for sample in speech:\n",
    "        len_speech = sample.shape[1]\n",
    "        sample_noise = torch.concat([noise[6],noise[6],noise[6],noise[6]],dim=1)# Repeat noise to ensure noise is longer than speech\n",
    "        sample_noise = torch.narrow(sample_noise,1,0,len_speech)# Shorten noise to same length as speech\n",
    "        x = sample_noise\n",
    "        X.append(x)\n",
    "    return X\n",
    "\n",
    "\n",
    "def prep_xij(trainX,i,j):\n",
    "    real_part = trainX[i][j].real\n",
    "    imag_part = trainX[i][j].imag\n",
    "    return torch.cat((real_part.unsqueeze(2),imag_part.unsqueeze(2)),2)\n",
    "\n",
    "# STFT\n",
    "N_FFT = 1024\n",
    "N_HOP = 256\n",
    "\n",
    "stft = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=N_HOP,\n",
    "    power=None,\n",
    ")\n",
    "istft = torchaudio.transforms.InverseSpectrogram(n_fft=N_FFT, hop_length=N_HOP)\n",
    "\n",
    "\n",
    "speech = load_speech()\n",
    "noise = load_noise()\n",
    "X = add_noise_to_speech(speech, noise)\n",
    "noise = get_one_noise(speech,noise)\n",
    "\n",
    "\n",
    "stfts_mix = []\n",
    "for x in X:\n",
    "    # Add second channel with double Amplitude\n",
    "    x2 = x*2\n",
    "    x_new = torch.concat([stft(x),stft(x2)],dim=0) # mel_stft\n",
    "    stfts_mix.append(x_new)\n",
    "    \n",
    "stfts_clean = []\n",
    "for y in speech:\n",
    "    y_new = stft(y)\n",
    "    y_new = y_new.reshape(513,-1)\n",
    "    stfts_clean.append(y_new)\n",
    "\n",
    "stfts_noise = []\n",
    "i = 0\n",
    "for n in noise:\n",
    "    try:\n",
    "        n_new = stft(n)\n",
    "        stfts_noise.append(n_new.reshape(513,-1))\n",
    "    except Exception: #sometimes noises are very short, e.g. noise[697]\n",
    "        continue\n",
    "\n",
    "trainX = stfts_mix\n",
    "\n",
    "print(stfts_clean[0].shape)\n",
    "print(stfts_mix[0][0].shape)\n",
    "print(stfts_noise[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irms(stft_clean, stft_noise):\n",
    "    mag_clean = stft_clean.abs() ** 2\n",
    "    mag_noise = stft_noise.abs() ** 2\n",
    "    irm_speech = mag_clean / (mag_clean + mag_noise)\n",
    "    irm_noise = mag_noise / (mag_clean + mag_noise)\n",
    "    return irm_speech[REFERENCE_CHANNEL], irm_noise[REFERENCE_CHANNEL]\n",
    "    \n",
    "trainY = []\n",
    "for n in range(0,len(noise)):\n",
    "    irm_speech, irm_noise = get_irms(stfts_clean[n].unsqueeze(0), stfts_noise[n].unsqueeze(0))\n",
    "    trainY.append(torch.cat((irm_speech.unsqueeze(0),irm_noise.unsqueeze(0)),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskNet + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASK NET\n",
    "HIDDEN_SIZE=64 # 128\n",
    "SAMPLE_RATE = 16000\n",
    "INPUT_CHANNEL = 2 # Always two -> Real and Imaginary part \n",
    "\n",
    "class MaskNet(Module):\n",
    "    def __init__(self,noise=False):\n",
    "        super(MaskNet, self).__init__()\n",
    "        # First subnet for speech prediction\n",
    "        self.noise = noise\n",
    "        self.lstm = LSTM(input_size=INPUT_CHANNEL, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True)\n",
    "        self.fc = Linear(in_features=HIDDEN_SIZE*2 ,out_features=1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Speech prediction\n",
    "        y, (h_n, c_n) = self.lstm(x)\n",
    "        y = self.fc(y)\n",
    "        speech_pred = self.sigmoid(y)\n",
    "\n",
    "        return speech_pred.reshape(513,-1)\n",
    "\n",
    "print(summary(MaskNet(),torch.zeros((513, 196, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "NUM_CHANNEL = 2 # Number of Mic Inputs (>=2 for BF)\n",
    "REFERENCE_CHANNEL = 0\n",
    "INIT_LR = 0.01\n",
    "BATCH_SIZE = 1#64\n",
    "LEARN_LOSS_PARAMS = False\n",
    "device =  torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = MaskNet()#.to(device)\n",
    "\n",
    "if LEARN_LOSS_PARAMS:\n",
    "    pass\n",
    "    #mse_wrapper = MultiTaskLossWrapper(task_num=4)\n",
    "else:\n",
    "    lossMSE = MSELoss() #MSELoss # Better Compare MEL instead MSE for both speech and noise\n",
    "\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "\n",
    "\n",
    "H = {\n",
    "    \"train_loss\":[],\n",
    "    \"train_acc\":[],\n",
    "    \"val_loss\":[],\n",
    "    \"val_acc\":[]\n",
    "}\n",
    "\n",
    "print(\"[INFO] training the network...\")\n",
    "#startTime = time.time()\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    print(\"Epoch:\",str(epoch)+\"/\"+str(EPOCHS))\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "    trainX = stfts_mix[:50]\n",
    "\n",
    "    for i in tqdm(range(0,len(trainX))): # Iterate over Training Examples\n",
    "        for j in range(0,NUM_CHANNEL):# + Iterate over channels\n",
    "            (x, y_s, y_n) = (prep_xij(trainX,i,j),trainY[i][0],trainY[i][1])\n",
    "            speech_pred=model(x)#, noise_pred = model(x)\n",
    "            if LEARN_LOSS_PARAMS:\n",
    "                pass\n",
    "                #loss = mse_wrapper([speech_pred_real, speech_pred_imag, noise_pred_real, noise_pred_imag],y_n.real,y_n.imag,y_s.real,y_s.imag)\n",
    "            else:\n",
    "                #noise_pred = torch.ones([196])-speech_pred\n",
    "                loss = lossMSE(speech_pred,y_s) #+ lossMSE(noise_pred,y_n)\n",
    "            # zero out the gradients, perform the backpropagation step, and update the weights\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            # add the loss to the total training loss so far and calculate the number of correct predictions\n",
    "            totalTrainLoss += loss\n",
    "    PATH = \"./modelsaveLibreOneNoise\"\n",
    "    torch.save(model.state_dict(), PATH + \"model_epoch\" + str(epoch) + \".pt\")\n",
    "    print(totalTrainLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./modelsaveLibreOneNoise750.pt\"\n",
    "model = MaskNet()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "\n",
    "def evaluate_example(e_nr):\n",
    "    model.eval()\n",
    "    speech_pred = model(prep_xij(trainX,e_nr,0))\n",
    "    noise_pred = torch.ones([513,196])-speech_pred\n",
    "    plot_mask(speech_pred, title=\"Prediction Speech\")\n",
    "    plot_mask(noise_pred, title=\"Prediction Noise\")\n",
    "    plot_mask(trainY[e_nr][0], title=\"Reference Speech\")\n",
    "    plot_mask(trainY[e_nr][1], title=\"Reference Noise\")\n",
    "\n",
    "def plot_mask(mask, title=\"Mask\", xlim=None):\n",
    "    mask = mask.detach().numpy()\n",
    "    figure, axis = plt.subplots(1, 1)\n",
    "    img = axis.imshow(mask, cmap=\"viridis\", origin=\"lower\", aspect=\"auto\")\n",
    "    figure.suptitle(title)\n",
    "    plt.colorbar(img, ax=axis)\n",
    "    plt.show()\n",
    "\n",
    "def maskToWave(speech_pred,noise_pred,mix):\n",
    "        model.eval()\n",
    "        psd_transform = torchaudio.transforms.PSD()\n",
    "        psd_speech = psd_transform(mix, speech_pred)\n",
    "        psd_noise = psd_transform(mix, noise_pred)\n",
    "        mvdr_transform = torchaudio.transforms.SoudenMVDR()\n",
    "        stft_souden = mvdr_transform(mix, psd_speech, psd_noise, reference_channel=REFERENCE_CHANNEL)\n",
    "        waveform_souden = istft(stft_souden, length=len(X[i][0]))#X[i].shape[-1])\n",
    "        return waveform_souden.reshape(-1)\n",
    "\n",
    "evaluate_example(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(i,sample_rate=SAMPLE_RATE):\n",
    "    noise_pred = torch.ones([513,196])-speech_pred\n",
    "    wave = maskToWave(speech_pred,noise_pred,stfts_mix[i])\n",
    "    torchaudio.save(\"sample_model_output.wav\",wave.reshape(1,-1),sample_rate)\n",
    "    torchaudio.save(\"sample_reference.wav\", speech[i].reshape(1,-1),sample_rate)\n",
    "    torchaudio.save(\"sample_input.wav\", X[i].reshape(1,-1),sample_rate)\n",
    "\n",
    "save_sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_pred = trainY[0][0]\n",
    "noise_pred = trainY[0][1]\n",
    "\n",
    "def evaluateSiSNR(wave, i):\n",
    "    def si_snr(estimate, reference, epsilon=1e-8):\n",
    "        estimate = estimate - estimate.mean()\n",
    "        reference = reference - reference.mean()\n",
    "        reference_pow = reference.pow(2).mean(axis=1, keepdim=True)\n",
    "        mix_pow = (estimate * reference).mean(axis=1, keepdim=True)\n",
    "        scale = mix_pow / (reference_pow + epsilon)\n",
    "\n",
    "        reference = scale * reference\n",
    "        error = estimate - reference\n",
    "\n",
    "        reference_pow = reference.pow(2)\n",
    "        error_pow = error.pow(2)\n",
    "\n",
    "        reference_pow = reference_pow.mean(axis=1)\n",
    "        error_pow = error_pow.mean(axis=1)\n",
    "\n",
    "        si_snr = 10 * torch.log10(reference_pow) - 10 * torch.log10(error_pow)\n",
    "        return si_snr.item()\n",
    "    score = si_snr(wave, speech[i])\n",
    "    print(f\"Si-SNR score: {score}\") \n",
    "    return score\n",
    "\n",
    "wave = maskToWave(speech_pred,noise_pred,stfts_mix,i)\n",
    "score = evaluateSiSNR(wave,0)#3.9097"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04f0c822f1f3471cfda557225f32a3325398c976884d8d093b8fd824f1bbe21a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
