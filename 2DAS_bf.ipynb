{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.signal import correlate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 142464640]) torch.Size([1, 142464640]) 16000 16000\n"
     ]
    }
   ],
   "source": [
    "path = \"/project/data_asr/CHiME5/data/CHiME5/audio/dev/\"\n",
    "\n",
    "# U = mic array with 4 channels, P = Person (2 channels in wav)\n",
    "data_1, sample_rate_1 = torchaudio.load(path+\"S02_U01.CH1.wav\")\n",
    "data_2, sample_rate_2 = torchaudio.load(path+\"S02_U02.CH1.wav\")\n",
    "print(data_1.size(),data_2.size(), sample_rate_1, sample_rate_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14246464\n"
     ]
    }
   ],
   "source": [
    "# Narrow data1 and data2\n",
    "data_1 = torch.narrow(data_1, 1, 0, int(data_1.size(dim=1)*0.1)) #1% = 2849292\n",
    "data_2 = torch.narrow(data_2, 1, 0, int(data_2.size(dim=1)*0.1))\n",
    "inputs = [data_1,data_2]\n",
    "print(data_1.size(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1424646]) torch.Size([1, 1424646])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0].size(), inputs[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beamforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_size_tensors(tensors: list):\n",
    "    chan = tensors[0].size(dim=0)\n",
    "    if tensors[0].size(dim=1)>tensors[1].size(dim=1):\n",
    "        diff = tensors[0].size()[1]-tensors[1].size()[1]\n",
    "        tensors[1] = torch.concat((tensors[1],torch.zeros([chan,diff])),1)\n",
    "    else:\n",
    "        diff = -tensors[0].size(dim=1) + tensors[1].size(dim=1)\n",
    "        tensors[0] = torch.concat((tensors[1],torch.zeros([chan,diff])),1)\n",
    "    return tensors\n",
    "\n",
    "def calculate_delays_and_weights(inputs):\n",
    "    delay = calc_best_delay(inputs)\n",
    "    return delay,[0.5,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 24]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better: Cross-Correlation -> Choose Delay that maximizes this!\n",
    "def calculate_GCCPHAT(signal_1: torch.Tensor,signal_2: torch.Tensor):\n",
    "    # torch Tensor to numpy array\n",
    "    signal_1 = signal_1.detach().numpy()\n",
    "    signal_2 = signal_2.detach().numpy()\n",
    "    # a is x_i b is x_ref\n",
    "    z = fft(signal_1)*np.conj(fft(signal_2))\n",
    "    n = np.absolute(fft(signal_1),fft(signal_2))\n",
    "    return ifft(z/n)\n",
    "\n",
    "def four_maxsum(array):\n",
    "    m1 = 0\n",
    "    m2 = 0\n",
    "    m3 = 0\n",
    "    m4 = 0\n",
    "    for a in array[0]:\n",
    "        if a > m4:\n",
    "            if a > m3:\n",
    "                if a > m2:\n",
    "                    if a > m1:\n",
    "                        m1 = a\n",
    "                    else:\n",
    "                        m2 = a\n",
    "                else:\n",
    "                    m3 = a\n",
    "            else:\n",
    "                m4 = a\n",
    "    return m1 + m2 + m3 + m4\n",
    "\n",
    "def calc_best_delay(inputs):\n",
    "    best_delay = 0\n",
    "    maxgcc = 0\n",
    "    delay_range = 100\n",
    "    for i in range(0,delay_range):\n",
    "        gcc = calculate_GCCPHAT(inputs[0][:,:1000],inputs[1][:,i:i+1000])\n",
    "        # calculate top 4 maxima of gccphat\n",
    "        maxsum = four_maxsum(gcc)\n",
    "        if maxsum > maxgcc:\n",
    "            maxgcc = maxsum\n",
    "            best_delay = i\n",
    "    return [0,best_delay]\n",
    "\n",
    "calc_best_delay(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights like DAS Beamformer in Anguera et al. (2007)\n",
    "def calculate_init_weights(inputs: list):\n",
    "    w = [[]]\n",
    "    #Computed at output, first:\n",
    "    for i in inputs:\n",
    "        channels = len(inputs)\n",
    "        w[0].append(1/channels)\n",
    "    return w\n",
    "\n",
    "def calculate_weight_update(inputs, w):\n",
    "    t = len(w)\n",
    "    corr = xcorr(inputs, t)\n",
    "    adapt_ratio = 0.05\n",
    "    #channels = len(w[0])\n",
    "    #helplist = []\n",
    "    #for i in range(0,channels):\n",
    "    #        helplist.append((1-adapt_ratio)*w[t-1][i]+adapt_ratio*corr)\n",
    "    #w.append(helplist)\n",
    "    w1 = (1-adapt_ratio)*w[t-1][0]+adapt_ratio*corr\n",
    "    w2 = 1-w1\n",
    "    w.append([w1,w2])\n",
    "    return w\n",
    "\n",
    "def xcorr(inputs, t):\n",
    "    #1 second blocks\n",
    "    try:\n",
    "        ch1 = torch.narrow(inputs[0], 1, (t-1)*16000, 16000)\n",
    "        ch2 = torch.narrow(inputs[1], 1, (t-1)*16000, 16000)\n",
    "        num_channels = len(inputs)\n",
    "        return (1/(16000*(num_channels))) * np.sum(signal.correlate(ch1,ch2))    \n",
    "    except Exception as e:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new sound waveform\n",
    "\n",
    "def DAS_bf(inputs: list, delays: list, weights_updated: bool):\n",
    "    output = torch.Tensor()\n",
    "    inputs = same_size_tensors(inputs)\n",
    "    w = calculate_init_weights(inputs)\n",
    "\n",
    "    max_time = inputs[0].size()[1]\n",
    "    num_channels = len(inputs)\n",
    "\n",
    "    for t in tqdm(range(0,max_time)):\n",
    "        output_at_t = torch.FloatTensor([0.0])\n",
    "        for i in range(0,num_channels):\n",
    "            try:\n",
    "                a = torch.multiply(w[-1][i],(inputs[i][:,t+delays[i]]))\n",
    "                output_at_t = torch.add(output_at_t,a)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        output = torch.cat((output,output_at_t),0)\n",
    "        if weights_updated and t % 16000 == 0:\n",
    "            w = calculate_weight_update(inputs,w)\n",
    "    output = output[None,:]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1424646/1424646 [01:56<00:00, 12207.84it/s]\n"
     ]
    }
   ],
   "source": [
    "delays = calc_best_delay(inputs)\n",
    "output = DAS_bf(inputs, delays, weights_updated=True)\n",
    "#print(output,output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"output.wav\", output, sample_rate_1)\n",
    "torchaudio.save(\"data1.wav\", data_1, sample_rate_1)\n",
    "torchaudio.save(\"data2.wav\", data_2, sample_rate_1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7edcfa7c72349f2a40b8bb9d00f805dd689758e4b70f704e502841c884b714f7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('beamformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
