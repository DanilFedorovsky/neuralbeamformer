{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "from torchmetrics import SignalNoiseRatio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Module, Linear, Sigmoid, LSTM, BCELoss, MSELoss, Conv2d, MaxPool2d\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from pytorch_model_summary import summary\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import speechbrain as sb\n",
    "from speechbrain.nnet.losses import get_si_snr_with_pitwrapper\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_CHANNEL = 0\n",
    "SAME_LENGTH = True\n",
    "THR_S = 0.5 # Threshold for speech IRM\n",
    "THR_N = 0.5\n",
    "N_PATH = \"/project/data_asr/CHiME5/data/librenoise/free-sound/\"#\"/Users/danilfedorovsky/Documents/10 Collection/00 Studium/00 Letztes Semester/Masterarbeit/Data/noise/free-sound/\"\n",
    "S_PATH = \"/project/data_asr/CHiME5/data/librenoise/dev/dev-clean/\"#\"/Users/danilfedorovsky/Documents/10 Collection/00 Studium/00 Letztes Semester/Masterarbeit/Data/LibriSpeech/dev-clean/\"\n",
    "MODEL_SAVE_PATH = \"/project/data_asr/CHiME5/data/librenoise/models/\"\n",
    "def load_noise(N_PATH=N_PATH):\n",
    "    noise = []\n",
    "    for file in  os.listdir(N_PATH):\n",
    "        if file[-4:] == \".wav\":\n",
    "            sound, _ = torchaudio.load(N_PATH+file)\n",
    "            noise.append(sound)\n",
    "    return noise\n",
    "\n",
    "\n",
    "def load_speech(S_PATH=S_PATH):\n",
    "    speech = []\n",
    "    for folder in  os.listdir(S_PATH):\n",
    "        if os.path.isdir(S_PATH+folder):\n",
    "            for subfolder in os.listdir(S_PATH+folder):\n",
    "                if os.path.isdir(S_PATH+folder+\"/\"+subfolder):\n",
    "                    for file in os.listdir(S_PATH+folder+\"/\"+subfolder):\n",
    "                        if file[-5:] == \".flac\":\n",
    "                            sound, _ = torchaudio.load(S_PATH+folder+\"/\"+subfolder+\"/\"+file)\n",
    "                            if SAME_LENGTH:\n",
    "                                try:\n",
    "                                    sound = torch.narrow(sound,1,0,50000)# Narrow to 50000\n",
    "                                except Exception:\n",
    "                                    # add zeros to make sound 50000 long\n",
    "                                    len_sound = sound.shape[1]\n",
    "                                    add_zeros = 50000 - len_sound\n",
    "                                    add_zeros = torch.zeros(add_zeros).reshape(1,-1)\n",
    "                                    sound = torch.concat([sound,add_zeros],dim=1)\n",
    "                            speech.append(sound)\n",
    "    return speech\n",
    "\n",
    "def add_noise_to_speech(speech, noise, ratio1: float, ratio2: float):\n",
    "    X = []\n",
    "    X2 = []\n",
    "    newNoise = []\n",
    "    for sample in speech:\n",
    "        len_speech = sample.shape[1]\n",
    "        sample_noise = random.choice(noise)\n",
    "        while sample_noise.shape[1]<len_speech:\n",
    "            sample_noise = torch.concat([sample_noise,sample_noise],dim=1)# Repeat to ensure noise is longer than speech\n",
    "        sample_noise = torch.narrow(sample_noise,1,0,len_speech)# Shorten noise to same length as speech\n",
    "        x = torch.add(sample,sample_noise*ratio1)# Same Ratio 1:1\n",
    "        x2 = torch.add(sample,sample_noise*ratio2)# Same Ratio 1:1\n",
    "        sample_noise = torch.narrow(sample_noise,1,0,len_speech)\n",
    "        X.append(x)\n",
    "        X2.append(x)\n",
    "        newNoise.append(sample_noise)\n",
    "    return X, X2, newNoise    \n",
    "\n",
    "def prep_xij(trainX,i,j):\n",
    "    real_part = trainX[i][j].real\n",
    "    imag_part = trainX[i][j].imag\n",
    "    return torch.cat((real_part.unsqueeze(2),imag_part.unsqueeze(2)),2)\n",
    "\n",
    "speech = load_speech()\n",
    "noise = load_noise()\n",
    "#noise = noise[:5]\n",
    "mix1, mix2, noise = add_noise_to_speech(speech, noise, 0.1, 0.3)\n",
    "\n",
    "print(len(speech), speech[0].shape)\n",
    "print(len(noise), noise[0].shape)\n",
    "print(len(mix1), mix2[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT\n",
    "N_FFT = 1024\n",
    "N_HOP = 256\n",
    "\n",
    "stft = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=N_HOP,\n",
    "    power=None,\n",
    ")\n",
    "istft = torchaudio.transforms.InverseSpectrogram(n_fft=N_FFT, hop_length=N_HOP)\n",
    "\n",
    "stfts_mix = []\n",
    "for n in range(0,len(mix1)):\n",
    "    x_new = torch.concat([stft(mix1[n]),stft(mix2[n])],dim=0)\n",
    "    stfts_mix.append(x_new)\n",
    "\n",
    "    \n",
    "stfts_clean = []\n",
    "for y in speech:\n",
    "    y_new = stft(y)\n",
    "    y_new = y_new.reshape(513,-1)\n",
    "    stfts_clean.append(y_new)\n",
    "\n",
    "stfts_noise = []\n",
    "i = 0\n",
    "for n in noise:\n",
    "    try:\n",
    "        n_new = stft(n)\n",
    "        stfts_noise.append(n_new.reshape(513,-1))\n",
    "    except Exception: #sometimes noises are very short, e.g. noise[697]\n",
    "        continue\n",
    "\n",
    "print(stfts_clean[0].shape)\n",
    "print(stfts_mix[0][0].shape)\n",
    "print(stfts_noise[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irms(stft_clean, stft_noise):\n",
    "    mag_clean = stft_clean.abs() ** 2\n",
    "    mag_noise = stft_noise.abs() ** 2\n",
    "    irm_speech = mag_clean / (mag_clean + mag_noise)\n",
    "    irm_noise = mag_noise / (mag_clean + mag_noise)\n",
    "    return irm_speech[REFERENCE_CHANNEL], irm_noise[REFERENCE_CHANNEL]\n",
    "\n",
    "Y = []\n",
    "for n in range(0,len(noise)):\n",
    "    irm_speech, irm_noise = get_irms(stfts_clean[n].unsqueeze(0), stfts_noise[n].unsqueeze(0))\n",
    "    irm_speech = (irm_speech>THR_S).float()\n",
    "    irm_noise = (irm_noise>THR_N).float()\n",
    "    Y.append(torch.cat((irm_speech.unsqueeze(0),irm_noise.unsqueeze(0)),0))\n",
    "\n",
    "X_h = []\n",
    "stfts_mix_s = torch.stack(stfts_mix)\n",
    "\n",
    "for i in range(0, len(stfts_mix)):\n",
    "    for j in range (0,1):\n",
    "        X_h.append(torch.cat((stfts_mix_s[i][j].real.unsqueeze(0),stfts_mix_s[i][j].imag.unsqueeze(0)),0))\n",
    "\n",
    "speech_s = torch.stack(speech)\n",
    "noise_s = torch.stack(noise)\n",
    "mix1_s = torch.stack(mix1)\n",
    "Y = torch.stack(Y)\n",
    "X = torch.stack(X_h)\n",
    "#Y = speech_s # NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskNet + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASK NET\n",
    "HIDDEN_SIZE=1024 # 1024 (128 is too litte, just learns all 0 or 1)\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "class MaskNet(Module):\n",
    "    def __init__(self,noise=False):\n",
    "        super(MaskNet, self).__init__()\n",
    "        # First subnet for speech prediction\n",
    "        self.conv1 = Conv2d(2, 392,kernel_size=(3,3),padding=(1,1)) # IN: 196x513x2 -> Out: 196x513x128\n",
    "        self.conv2 = Conv2d(392, 392,kernel_size=(3,3),padding=(1,1))\n",
    "        self.maxpool = MaxPool2d((2,2),stride=2)\n",
    "        self.conv3 = Conv2d(392, 196,kernel_size=(3,3),padding=(1,1))\n",
    "        self.conv4 = Conv2d(196, 196,kernel_size=(3,3),padding=(1,1))\n",
    "        self.lstm = LSTM(input_size=196, hidden_size=HIDDEN_SIZE, num_layers=2, bidirectional=True)\n",
    "        self.fc = Linear(in_features=HIDDEN_SIZE*2 ,out_features=1024)\n",
    "        self.fc2 = Linear(in_features=1024 ,out_features=1024)\n",
    "        self.fc3 = Linear(in_features=1024 ,out_features=1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Speech prediction\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #self.lstm.flatten_parameters()\n",
    "        x, (h_n, c_n) = self.lstm(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        speech_pred = self.sigmoid(x)\n",
    "        return speech_pred.reshape(1, 513, 196)#, noise_pred\n",
    "\n",
    "print(summary(MaskNet(),torch.zeros((2, 513, 196))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "REFERENCE_CHANNEL = 0\n",
    "INIT_LR = 0.01\n",
    "PICKLE_SAVE_PATH = '/project/data_asr/CHiME5/data/librenoise/models/params.pkl'\n",
    "MODEL_SAVE_PATH = '/project/data_asr/CHiME5/data/librenoise/models/modelLibre'\n",
    "\n",
    "CUDA = True # if torch.cuda.is_available()\n",
    "device =  torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Mounted on:\", device)\n",
    "\n",
    "lossBCE = BCELoss().to(device)\n",
    "\n",
    "model = MaskNet().to(device)\n",
    "model= torch.nn.DataParallel(model,device_ids=[0])\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "\n",
    "H = {\n",
    "    \"train_loss\":[],\n",
    "    \"train_acc\":[],\n",
    "    \"val_loss\":[],\n",
    "    \"val_acc\":[]\n",
    "}\n",
    "\n",
    "def check_accuracy_training(speech_pred, y_s):\n",
    "    speech_pred = (speech_pred>0.15).float()\n",
    "    return float(torch.sum((speech_pred == y_s).float())/torch.sum(torch.ones(513,speech_pred.shape[1])))\n",
    "\n",
    "def check_accuracy_validation(model):\n",
    "    example_nr = int(np.random.random()*(len(speech)-len(trainX))+len(trainX))\n",
    "    model.eval()\n",
    "    pred = model(X[example_nr]).reshape(1,513,-1)\n",
    "    val_loss = lossBCE(pred,Y[example_nr][0].unsqueeze(0))\n",
    "    pred = (pred>0.15).float()\n",
    "    model.train()\n",
    "    return float(torch.sum((pred == Y[example_nr][0]).float())/torch.sum(torch.ones(513,X[example_nr].shape[2])).to(device)),val_loss\n",
    "\n",
    "print(\"[INFO] training the network...\")\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    print(\"Epoch:\",str(epoch+1)+\"/\"+str(EPOCHS))\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    trainX = X[:2000]\n",
    "    trainY = Y\n",
    "    for i in tqdm(range(0,len(trainX))): # Iterate over Training Examples\n",
    "        (x, y) = (trainX[i],trainY[i][0].unsqueeze(0))\n",
    "        speech_pred=model(x)\n",
    "        loss = lossBCE(speech_pred,y)\n",
    "        # zero out the gradients, perform the backpropagation step, and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        #H[\"train_acc\"].append(check_accuracy_training(speech_pred,y))\n",
    "        H[\"train_acc\"].append(check_accuracy_training(speech_pred,y))\n",
    "        H[\"train_loss\"].append(float(loss))\n",
    "        if i % 10 == 0:\n",
    "            val_acc, val_loss = check_accuracy_validation(model)\n",
    "            H[\"val_acc\"].append(val_acc)\n",
    "            H[\"val_loss\"].append(float(val_loss))\n",
    "        if i % 100 == 0:\n",
    "            if i == 0:\n",
    "                continue\n",
    "            print(\"Average Training Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"train_acc\"])))\n",
    "            print(\"Total Training Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"train_loss\"])))\n",
    "            print(\"Average Validation Accuracy at Iteration\",str(i),\":\",np.mean(np.array(H[\"val_acc\"])))\n",
    "            print(\"Total Validation Loss at Iteration\",str(i),\":\",np.sum(np.array(H[\"val_loss\"])))\n",
    "    # Save\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH + \"epoch\"+ str(epoch+1) + \".pt\")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH + \"final\" + \".pt\")\n",
    "with open(PICKLE_SAVE_PATH, 'wb') as f:\n",
    "    pickle.dump(H, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_SAVE_PATH = '/project/data_asr/CHiME5/data/librenoise/models/params.pkl'\n",
    "\n",
    "with open(PICKLE_SAVE_PATH, 'rb') as f:\n",
    "    H = pickle.load(f)\n",
    "\n",
    "n = 10\n",
    "plt.plot(H[\"train_loss\"][::n*10])\n",
    "plt.plot(H[\"val_loss\"][::n])\n",
    "plt.legend(['Training Loss', 'Validation Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"modelLibrefinal.pt\"\n",
    "model = MaskNet().cpu()\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH+MODEL_NAME),strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(stft, title=\"Spectrogram\", xlim=None):\n",
    "    magnitude = stft.abs()\n",
    "    spectrogram = 20 * torch.log10(magnitude + 1e-8).detach().numpy()\n",
    "    figure, axis = plt.subplots(1, 1)\n",
    "    print(spectrogram.shape)\n",
    "    spectrogram = spectrogram.reshape(513,196)\n",
    "    print(spectrogram.shape)\n",
    "    img = axis.imshow(spectrogram, cmap=\"viridis\", vmin=-100, vmax=0, origin=\"lower\", aspect=\"auto\")\n",
    "    figure.suptitle(title)\n",
    "    plt.colorbar(img, ax=axis)\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_example(e_nr):\n",
    "    model.eval()\n",
    "    #speech_pred = (model(X[e_nr]).reshape(513,-1)>0.2).float()\n",
    "    speech_mask = model(X[e_nr]).reshape(513,-1)\n",
    "    #noise_mask = torch.ones([513,speech_pred.shape[1]])-speech_mask\n",
    "    speech_pred=maskToWave(speech_mask,e_nr)\n",
    "    noise_pred=mix1[e_nr]-speech_pred\n",
    "    #speech_pred = model(X[e_nr]).reshape(513,-1)\n",
    "    plot_spectrogram(stft(speech_pred), title=\"Prediction Speech\")\n",
    "    plot_spectrogram(stft(noise_pred), title=\"Prediction Noise\")\n",
    "    plot_spectrogram(stft(speech_s[e_nr]), title=\"Reference Speech\")\n",
    "    plot_spectrogram(stft(noise_s[e_nr]), title=\"Reference Noise\")\n",
    "\n",
    "def plot_mask(mask, title=\"Mask\", xlim=None):\n",
    "    mask = mask.cpu().detach().numpy()\n",
    "    figure, axis = plt.subplots(1, 1)\n",
    "    img = axis.imshow(mask, cmap=\"viridis\", origin=\"lower\", aspect=\"auto\")\n",
    "    figure.suptitle(title)\n",
    "    plt.colorbar(img, ax=axis)\n",
    "    plt.show()\n",
    "\n",
    "istft = torchaudio.transforms.InverseSpectrogram(n_fft=N_FFT, hop_length=N_HOP)\n",
    "\n",
    "def maskToWave(speech_pred,i):\n",
    "        mix = stfts_mix[i]\n",
    "        noise_pred = torch.ones([513,mix.shape[2]])-speech_pred\n",
    "        psd_transform = torchaudio.transforms.PSD()\n",
    "        psd_speech = psd_transform(mix, speech_pred)\n",
    "        psd_noise = psd_transform(mix, noise_pred)\n",
    "        mvdr_transform = torchaudio.transforms.SoudenMVDR()\n",
    "        stft_souden = mvdr_transform(mix, psd_speech, psd_noise, reference_channel=REFERENCE_CHANNEL)\n",
    "        waveform_souden = istft(stft_souden, length=50000)#X[i].shape[-1])\n",
    "        return waveform_souden.unsqueeze(0)\n",
    "\n",
    "evaluate_example(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_pred = trainY[0][0]\n",
    "noise_pred = trainY[0][1]\n",
    "\n",
    "def evaluateSiSNR(wave, i):\n",
    "    def si_snr(estimate, reference, epsilon=1e-8):\n",
    "        estimate = estimate - estimate.mean()\n",
    "        reference = reference - reference.mean()\n",
    "        reference_pow = reference.pow(2).mean(axis=1, keepdim=True)\n",
    "        mix_pow = (estimate * reference).mean(axis=1, keepdim=True)\n",
    "        scale = mix_pow / (reference_pow + epsilon)\n",
    "\n",
    "        reference = scale * reference\n",
    "        error = estimate - reference\n",
    "\n",
    "        reference_pow = reference.pow(2)\n",
    "        error_pow = error.pow(2)\n",
    "\n",
    "        reference_pow = reference_pow.mean(axis=1)\n",
    "        error_pow = error_pow.mean(axis=1)\n",
    "\n",
    "        si_snr = 10 * torch.log10(reference_pow) - 10 * torch.log10(error_pow)\n",
    "        return si_snr.item()\n",
    "    score = si_snr(wave, speech[i])\n",
    "    print(f\"Si-SNR score: {score}\") \n",
    "    return score\n",
    "\n",
    "def maskToWave(speech_pred,noise_pred,mix,i):\n",
    "        model.eval()\n",
    "        psd_transform = torchaudio.transforms.PSD()\n",
    "        psd_speech = psd_transform(mix[i], speech_pred)\n",
    "        psd_noise = psd_transform(mix[i], noise_pred)\n",
    "        mvdr_transform = torchaudio.transforms.SoudenMVDR()\n",
    "        stft_souden = mvdr_transform(mix[i], psd_speech, psd_noise, reference_channel=REFERENCE_CHANNEL)\n",
    "        waveform_souden = istft(stft_souden, length=len(X[i][0]))#X[i].shape[-1])\n",
    "        return waveform_souden.reshape(-1)\n",
    "\n",
    "def save_sample(i,wave,sample_rate=SAMPLE_RATE):\n",
    "    model.eval()\n",
    "    wave = wave\n",
    "    torchaudio.save(\"sampleall_output.wav\",wave.reshape(1,-1),sample_rate)\n",
    "    torchaudio.save(\"sampleall_reference.wav\", speech[i].reshape(1,-1),sample_rate)\n",
    "    torchaudio.save(\"sampleall_input.wav\", X[i].reshape(1,-1),sample_rate)\n",
    "\n",
    "\n",
    "sample_nr = 51\n",
    "speech_pred = (model(prep_xij(stfts_mix,sample_nr,0))>0.15).float()\n",
    "noise_pred = torch.ones([513,speech_pred.shape[1]])-speech_pred\n",
    "wave = maskToWave(speech_pred,noise_pred,stfts_mix,sample_nr)\n",
    "save_sample(sample_nr, wave=wave)\n",
    "score = evaluateSiSNR(wave,sample_nr)#Should be ~16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('beamformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7edcfa7c72349f2a40b8bb9d00f805dd689758e4b70f704e502841c884b714f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
